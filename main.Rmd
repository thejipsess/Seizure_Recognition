---
title: "Seizure Recognition"
author: "Jip de Kok"
date: "9/15/2020"
output:
  html_document: default
  pdf_document: default
---

The data used here is EEG data. Originally there were 500 participants, who
were recorded for 23.5 seconds with 4097 data points per sample. Each sample
was divided into 23 chunks , each containing 178 data points for 1 second.
These chunks were shuffled, resulting in 11500 samples.


5 - (healthy, surface recording) eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open
4 - (healthy, surface recording) eyes closed, means when they were recording the EEG signal the patient had their eyes closed
3 - (epiliptic, intracranial recording) The EEG activity from the hippocampal formation in the non-epiletogenic hemisphere with no seizure
2 - (epiliptic, intracranial recording) The EEG activity from the epileptogenic zone brain area with no seizure
1 - (epiliptic, intracranial recording) Recording of seizure activity



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This section loads all the the required packages and the data. Also, it removes missing data and splits the data into dependant and independant data.
```{r data, echo=FALSE, message = FALSE}
# Set seed to ensure reproducibility
set.seed(4685)

# Load packages.R to install and load all the required packages and files
source('packages.R')

# Load data
x = read.csv('Data/data.csv', sep = ';', row.names = 'X')
# Remove rows with missing data
x <- na.omit(x)

# Randomly permute y to create randomised data
permute <- FALSE
if(permute){
  set.seed(4685)
  x$y <- sample(x$y)}
  
# Isolate y
y <- x$y

# remove y column from x
x <- subset(x, select = -y)

```

```{r Data_Inspection}
# Count missing data
nullsum = sum(!complete.cases(x))

theme_set(theme_bw(base_size=12)+ 
  theme(panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()))

# Plot a random sample over time
plot_rand_sample(x)

```

```{r PCA}
# Perform PCA
pca <- prcomp(x, center = TRUE, scale=FALSE)

# Plot PCA
plot_pca(pca, y)

# Remove the sample with class 70
x <- x[y != 70,]
y <- y[y != 70]

```

```{r Plot_data_before}
# Create density plot of unnormalised data
plot_dens(x, "Data distribution (not pre-processed)")

# Make violin plots for the different classes
plot_violin(x, y, title = "violin plot for all classes (not pre-processed)")

# Create boxplots
plot_boxplot(x, y)
```


```{r Preprocessing}

# Mean center and scale (divide by stdev) the data
#preproc <- preProcess(x, method=c('center', 'scale'))
#x <- predict(preproc, x)

# Get rid of the most extreme outlier
row_max <- which(x == max(x), arr.ind = TRUE)[1]
y <- y[-row_max]
x <- x[-row_max,]


# Remove univariate outliers
for(i in seq(1, 178)){
  Q1 <- quantile(x[,i], 0.25)
  Q3 <- quantile(x[,i], 0.75)
  IQR <- Q3 - Q1
  uplim <- Q3 + 10*IQR
  downlim <- Q1 - 10*IQR
  
  y <- y[x[names(x)[i]] <= uplim & x[names(x)[i]] >= downlim]
  x <- x[x[names(x)[i]] <= uplim & x[names(x)[i]] >= downlim,]
}

# mean-center: Set mean of each variable to 0
# Scale: Unit variance scaling
preproc <- preProcess(x, method=c('center')) # Can also add 'scale'
x_norm <- predict(preproc, x)

# Min Max normalisation
for (col in colnames(x_norm)){
  x_norm[col] <- ((x_norm[col] - min(x_norm[col])) / (max(x_norm[col]) - min(x_norm[col])))
}

```


```{r Plot_data_after}
# Create density plot of unnormalised data
plot_dens(x_norm, "Data distribution (pre-processed)")

# Make violin plots for the different classes
plot_violin(x_norm, y, title = "violin plot for all classes (not pre-processed)")

# Create boxplots
plot_boxplot(x_norm, y)
```


``` {r distance, message=FALSE}
# create matrix z which contains both x and y, only for classes one and two
z <- x_norm
z$y <- y
z <- z[y == 1 | y == 2,]
z <- arrange(z, y)

# Calculate the distance matrix (euclidean distance)
distance <- dist(z[,-ncol(z)])
distance <- as.matrix(distance)

# Plot heatmap of the distance matrix
plot_heatmap(distance, save = TRUE, title = "Distance heatmap - no scaling")

# Perform double centring on the distance matrix
Rmean <- distance*0 + rowMeans(distance)
Cmean <- distance*0 + colMeans(distance)
distance = distance - Rmean - Cmean + mean(distance)

# Plot heatmap of the double centered distance matrix
plot_heatmap(distance, save = TRUE, title = "Distance heatmap - double centered")

# Perform PCA on normal data for class 1 & 2
pca <- prcomp(z[,-ncol(z)], center = TRUE, scale=FALSE)

# Plot PCA plots including 3D score plot
plot_pca(pca, y = z$y, title = "on class 1 & 2", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)

# Perform PCA on euclidean distance for class 1 & 2
pca_dist <- prcomp(distance, center = TRUE, scale=FALSE)

# Open new RGL device to allow both 3D plots to be active simultaneously
open3d()

# Plot PCA plots including 3D score plot
plot_pca(pca_dist, y = z$y, title = "on euclidean distance", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)

# Export 3D RGL device to html
writeWebGL(dir = "Figures", filename = "3d PCA.html")
```

```{r gaussian_kernel}

rbfkernel <- rbfdot(sigma = 0.1)

f <- kernelMatrix(rbfkernel, as.matrix(subset(z, select = -y)))

pca <- prcomp(f, center = TRUE, scale=FALSE)

plot_pca(pca, y = z$y, title = "kernel", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)

```

```{r gridsearch_rbf}
# Split the data into train and test set using the function from 'Classification.R'
split_data <- split_test_train(x, y, classes = c(1, 2))

# Balance the data by random oversampling
# Determine the number of of samples in the biggest class
max_train_class <- max(table(split_data$train$y))
max_test_class <- max(table(split_data$test$y))

# Oversample the minority class(es)
x_bin_train <- ovun.sample(y ~ ., split_data$train, method = 'over', N = max_train_class * length(classes), )$data
#x_bin_test <- ovun.sample(y ~ ., split_data$test, method = 'over', N = max_test_class * length(classes), )$data
x_bin_test <- split_data$test

# Check  if test and training set are balanced
print("Class balance of training set:")
print(table(x_bin_train$y)/length(x_bin_train$y)*100)
print("Class balance of test set:")
print(table(x_bin_test$y)/length(x_bin_test$y)*100)

# Create preprocessing recipe
SVM_recipe <-
    # Specify the recipe function, here the variable you want to predict should be put the left of the ~ sign,
    # and the predictor variables go to the right of the ~ symbol, since we want to use all other variables we use a dot (y ~ .)
    # We also specify data = x_bin_train such that the recipe will be based on the training data and we can apply an identical
    # transformation to the test set later on, and thus also to any other future data the model shoul be run on.
    recipe(y ~ ., data = x_bin_train) %>%
    # Set the mean of all predictor variables to 0
    step_center(all_predictors()) %>%
    # Here we fix the range of all the predictor variables of 0 to 1. (min-max normalisation)
    step_range(all_predictors(), min = 0, max = 1)

# Perform gridsearch
gridsearch_result <- gridsearch_svm_rbf(x_bin_train, SVM_recipe)

```

```{r SVM_rbg}
# Set seed to ensure reproducibility
set.seed(4685)

# Create svm model
svm <- svm_rbf(mode = "classification", cost = 5.004239542, rbf_sigma = 0.03505893)

# Fit the SVM model on t he training data
svm_fit <- 
  svm %>%
  set_engine("kernlab") %>%
  fit(y ~ ., x_bin_train)

# Evaluate the SVM model on the test data
test_results_rbf <- 
  x_bin_test$y %>%
  bind_cols(
    predict(svm_fit, x_bin_test)) %>%
  setNames(c('True class', 'Predicted class'))
  
# Compute the confusion matrix and evaluate the SVM performance
caret::confusionMatrix(factor(test_results_rbf[['Predicted class']]), factor(test_results_rbf[['True class']]))

# Generate PCA plot with svm misclassification information
correctly_classified_test <- test_results_rbf$'Predicted class' == test_results_rbf$'True class'

# PLot PCA of test sete with labels for misclassification
SVM_recipe <- prep(SVM_recipe, x_bin_train)
train_norm <- bake(SVM_recipe, x_bin_train)
test_norm <- bake(SVM_recipe, x_bin_test)
pca <- prcomp(subset(test_norm, select = -y), center = F, scale = F)
plot_pca(pca, correctly_classified_test, alpha = 0.5, title = "Test set with misclassifications", legend_title = 'Correct prediction')

# Plot PCA of training set and project test set onto it (this function is in  'plotter.R')
pca_projection(x_bin_train, x_bin_test, SVM_recipe)

# Plot density plots of the training and test before and after preprocessing
plot_dens(data.frame(subset(test_norm, select = -y)))
plot_dens(data.frame(subset(train_norm, select = -y)))
plot_dens(data.frame(subset(x_bin_train, select = -y)))
plot_dens(data.frame(subset(x_bin_test, select = -y)))
```
```{r gridsearch_lin}
# Set cost values to evaluate
cost_range = seq(1, 50, by = 4)
cost_range = 10^(-2:1)

# Perform gridsearch
model_opt <- gridsearch_svm_lin(train_norm, SVM_recipe, cost_range)


#model_opt = tune.svm(y ~ ., data = train_norm, type = "nu-classification", kernel = 'linear', cost = 10^(-1:2))
#model_opt = tune.svm(y ~ ., data = train_norm, type = "C-classification", kernel = 'polynomial', cost = 10^(-1:2))

```


```{r SVM_lin}
# Set seed to ensure reproducibility
set.seed(4685)

# Create svm model
svm_lin <- svm(y ~ ., train_norm, scale = FALSE, type = "nu-classification", kernel = 'linear',  cost = 0.1, probability = T)

pred <- predict(svm_lin, train_norm)

table(pred, train_norm$y)

# Evaluate the SVM model on the test data
test_results_lin <- 
  test_norm$y %>%
  bind_cols(
    predict(svm_lin, test_norm)) %>%
  setNames(c('True class', 'Predicted class'))

# Compute the confusion matrix and evaluate the SVM performance
caret::confusionMatrix(factor(test_results_lin[['Predicted class']]), factor(test_results_lin[['True class']]))



# Generate PCA plot with svm misclassification information
pca <- prcomp(x_bin_test, center = TRUE, scale=FALSE)
correctly_classified <- test_results_lin$'Predicted class' == test_results_lin$'True class'

x_bin_test_norm <- bake(SVM_recipe, x_bin_test)

f <- kernelMatrix(rbfkernel, as.matrix(x_bin_test_norm))

pca <- prcomp(f, center = TRUE, scale=TRUE)

plot_pca(pca, correctly_classified)


```

```{R roc}
# Predict probabilities on test set with rbf svm model
test_results_rbf <- 
  x_bin_test$y %>%
  bind_cols(
    predict(svm_fit, x_bin_test, type = 'prob')) %>%
  setNames(c('True class', 'Predicted class'))

# Predict probabilities on test set with linear svm model
test_results_lin <- 
  x_bin_test$y %>%
  bind_cols(
    attr(predict(svm_lin, x_bin_test, probability = T), 'probabilities')[,1]) %>%
  setNames(c('True class', 'Predicted class'))

# Create ROC curve for the linear and rbf svm models
par(pty = 's')
roc(as.numeric(test_results_rbf$'True class'), as.numeric(test_results_rbf$'Predicted class'), plot = T, legacy.axes = T,
     percent = T, xlab = "False postive %", ylab = "True positive percentage", print.auc = T, print.auc.y = 95, col = "cadetblue3")
plot.roc(as.numeric(test_results_lin$'True class'), as.numeric(test_results_lin$'Predicted class'), percent = T,
          print.auc = T, add = T, smoothed = F, col = "coral3")
legend(39, 20, legend=c("rbf", "linear"),
       col=c("cadetblue3", "coral3"), lty=1:2, cex=0.8)

```