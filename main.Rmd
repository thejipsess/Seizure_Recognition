---
title: "Seizure Recognition"
author: "Jip de Kok"
date: "9/15/2020"
output:
  html_document: default
  pdf_document: default
---

The data used here is EEG data. Originally there were 500 participants, who
were recorded for 23.5 seconds with 4097 data points per sample. Each sample
was divided into 23 chunks , each containing 178 data points for 1 second.
These chunks were shuffled, resulting in 11500 samples.


5 - (healthy, surface recording) eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open
4 - (healthy, surface recording) eyes closed, means when they were recording the EEG signal the patient had their eyes closed
3 - (epiliptic, intracranial recording) The EEG activity from the hippocampal formation in the non-epiletogenic hemisphere with no seizure
2 - (epiliptic, intracranial recording) The EEG activity from the epileptogenic zone brain area with no seizure
1 - (epiliptic, intracranial recording) Recording of seizure activity



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This section loads all the the required packages and the data. Also, it removes missing data and splits the data into dependant and independant data.
```{r data, echo=FALSE, message = FALSE}
source('packages.R')

# Load data
x = read.csv('Data/data.csv', sep = ';', row.names = 'X')
# Remove rows with missing data
x <- na.omit(x)

# Permute y to create random data
permute <- FALSE
if(permute){
  set.seed(4685)
  x$y <- sample(x$y)}
  
# Isolate y
y <- x$y

# remove y column from x
x <- subset(x, select = -y)

```

```{r Data_Inspection}
# Count missing data
nullsum = sum(!complete.cases(x))

theme_set(theme_bw(base_size=12)+ 
  theme(panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()))

# Plot a random sample over time
plot_rand_sample(x)

```

```{r PCA}
# Perform PCA
pca <- prcomp(x, center = TRUE, scale=FALSE)

# Plot PCA
plot_pca(pca, y)

# Remove the sample with class 70
x <- x[y != 70,]
y <- y[y != 70]

```

```{r Plot_data_before}
# Create density plot of unnormalised data
plot_dens(x, "Data distribution (not pre-processed)")

# Make violin plots for the different classes
plot_violin(x, y, title = "violin plot for all classes (not pre-processed)")

# Create boxplots
plot_boxplot(x, y)
```


```{r Preprocessing}

# Mean center and scale (divide by stdev) the data
#preproc <- preProcess(x, method=c('center', 'scale'))
#x <- predict(preproc, x)

# Get rid of the most extreme outlier
row_max <- which(x == max(x), arr.ind = TRUE)[1]
y <- y[-row_max]
x <- x[-row_max,]


# Remove univariate outliers
for(i in seq(1, 178)){
  Q1 <- quantile(x[,i], 0.25)
  Q3 <- quantile(x[,i], 0.75)
  IQR <- Q3 - Q1
  uplim <- Q3 + 10*IQR
  downlim <- Q1 - 10*IQR
  
  y <- y[x[names(x)[i]] <= uplim & x[names(x)[i]] >= downlim]
  x <- x[x[names(x)[i]] <= uplim & x[names(x)[i]] >= downlim,]
}

# mean-center: Set mean of each variable to 0
# Scale: Unit variance scaling
preproc <- preProcess(x, method=c('center')) # Can also add 'scale'
x_norm <- predict(preproc, x)

# Min Max normalisation
for (col in colnames(x_norm)){
  x_norm[col] <- ((x_norm[col] - min(x_norm[col])) / (max(x_norm[col]) - min(x_norm[col])))
}

```


```{r Plot_data_after}
# Create density plot of unnormalised data
plot_dens(x_norm, "Data distribution (pre-processed)")

# Make violin plots for the different classes
plot_violin(x_norm, y, title = "violin plot for all classes (not pre-processed)")

# Create boxplots
plot_boxplot(x_norm, y)
```


``` {r distance, message=FALSE}
# create matrix z which contains both x and y, only for classes one and two
z <- x_norm
z$y <- y
z <- z[y == 1 | y == 2,]
z <- arrange(z, y)

# Calculate the distance matrix (euclidean distance)
distance <- dist(z[,-ncol(z)])
distance <- as.matrix(distance)

# Plot heatmap of the distance matrix
plot_heatmap(distance, save = TRUE, title = "Distance heatmap - no scaling")

# Perform double centring on the distance matrix
Rmean <- distance*0 + rowMeans(distance)
Cmean <- distance*0 + colMeans(distance)
distance = distance - Rmean - Cmean + mean(distance)

# Plot heatmap of the double centered distance matrix
plot_heatmap(distance, save = TRUE, title = "Distance heatmap - double centered")

# Perform PCA on normal data for class 1 & 2
pca <- prcomp(z[,-ncol(z)], center = TRUE, scale=FALSE)

# Plot PCA plots including 3D score plot
plot_pca(pca, y = z$y, title = "on class 1 & 2", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)

# Perform PCA on euclidean distance for class 1 & 2
pca_dist <- prcomp(distance, center = TRUE, scale=FALSE)

# Open new RGL device to allow both 3D plots to be active simultaneously
open3d()

# Plot PCA plots including 3D score plot
plot_pca(pca_dist, y = z$y, title = "on euclidean distance", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)
```

```{r gaussian_kernel}

rbfkernel <- rbfdot(sigma = 0.1)

f <- kernelMatrix(rbfkernel, as.matrix(subset(z, select = -y)))

pca <- prcomp(f, center = TRUE, scale=FALSE)

plot_pca(pca, y = z$y, title = "kernel", save=TRUE, include_3D = TRUE, PC_x = 1, PC_y = 2, PC_z=3)

```

```{r gridsearch}
# Set x_bin (x binary) to the unnormalised x dataframe
x_bin <- x
# Attach the classes (y) to x_bin
x_bin$y <- y
# Only keep class 1 & 2
x_bin <- x_bin[y == 1 | y == 2,]
# Order the rows on the y column
x_bin <- arrange(x_bin, y)

# Change y into factor
x_bin$y <- factor(x_bin$y)

# Split data in training and test set
data_split <- initial_split(x_bin, strata = "y", p = 0.9)
x_bin_train <- training(data_split)
x_bin_test <- testing(data_split)


# Create SVM model with hyperparameter tuning capabilities
svm <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Setup bootstrapping resample data x_bin_rs
x_bin_rs <- bootstraps(x_bin_train, times = 100)

# Setup parallel computing
# Detect number of available threads (to use number of cores set logical to FALSE)
all_cores <- parallel::detectCores(logical = TRUE)
# Generate one R process for each core
cl <- makePSOCKcluster(all_cores)
# Register the paralel backedn for the cluster of processes
registerDoParallel(cl)

SVM_recipe <-
  # Specify the recipe function, here the variable you want to predict should be put the left of the ~ sign,
  # and the predictor variables go to the right of the ~ symbol, since we want to use all other variables we use a dot (y ~ .)
  # We also specify data = x_bin_train such that the recipe will be based on the training data and we can apply an identical
  # transformation to the test set later on, and thus also to any other future data the model shoul be run on.
  recipe(y ~ ., data = x_bin_train) %>%
    # Set the mean of all predictor variables to 0
    step_center(all_predictors()) %>%
    # Here we fix the range of all the predictor variables of 0 to 1. (min-max normalisation)
    step_range(all_predictors(), min = 0, max = 1)

# Prepare the recipe on the training data so it can also be applied to other data
SVM_recipe <- prep(SVM_recipe, x_bin_train)

# Set options for the grid search
ctrl <- control_grid(verbose = TRUE)

# Set the metric to optimise
metric <- metric_set(accuracy)

# Start timer
tic()

recipe_res <-
  svm %>% 
  tune_grid(
    SVM_recipe,
    resamples = x_bin_rs,
    grid = 10,
    metrics = metric,
    control = ctrl
  )

# show how long it took to optimise the svm model
toc()

recipe_res

# Stop the parallelisation
stopCluster(cl)


show_best(recipe_res, metric = "accuracy")

```

```{r SVM_rbg}
# Set seed to ensure reproducibility
set.seed(4685)

# Split data in training and test set
data_split <- initial_split(x_bin, strata = "y", p = 0.9)
x_bin_train <- training(data_split)
x_bin_test <- testing(data_split)

# Remove y from the training and test set
x_bin_train_y <- x_bin_train$y
x_bin_test_y <- x_bin_test$y
x_bin_train <- subset(x_bin_train, select = -y)
x_bin_test <- subset(x_bin_test, select = -y)

# Create svm model
svm <- svm_rbf(mode = "classification", cost = 5.004239542, rbf_sigma = 0.03505893)

# Fit the SVM model on t he training data
svm_fit <- 
  svm %>%
  set_engine("kernlab") %>%
  fit_xy(
    x = x_bin_train,
    y = factor(x_bin_train_y)
  )

# Evaluate the SVM model on the test data
test_results <- 
  x_bin_test_y %>%
  bind_cols(
    predict(svm_fit, new_data = x_bin_test)) %>%
  setNames(c('True class', 'Predicted class'))
  

# Compute the confusion matrix and evaluate the SVM performance
caret::confusionMatrix(factor(test_results[['Predicted class']]), factor(test_results[['True class']]))

# Generate PCA plot with svm misclassification information
pca <- prcomp(x_bin_test, center = TRUE, scale=FALSE)
correctly_classified <- test_results$'Predicted class' == test_results$'True class'

# Prepare the recipe
SVM_recipe <- prep(SVM_recipe)

x_bin_test_norm <- bake(SVM_recipe, x_bin_test)

f <- kernelMatrix(rbfkernel, as.matrix(x_bin_test_norm))

pca <- prcomp(f, center = TRUE, scale=TRUE)

plot_pca(pca, correctly_classified)

```


```{r SVM_lin}
# Set seed to ensure reproducibility
set.seed(4685)

# Split data in training and test set
data_split <- initial_split(x_bin, strata = "y", p = 0.9)
x_bin_train <- training(data_split)
x_bin_test <- testing(data_split)

# Remove y from the training and test set
x_bin_train_y <- x_bin_train$y
x_bin_test_y <- x_bin_test$y
x_bin_train <- subset(x_bin_train, select = -y)
x_bin_test <- subset(x_bin_test, select = -y)

# Create svm model
svm <- svm(x_bin_train, x_bin_train_y, scale = FALSE, type = "nu-classification", kernel = 'linear',  cost = 1)


pred <- predict(svm, x_bin_train)

table(pred, x_bin_train_y)

# Fit the SVM model on t he training data
svm_fit <- 
  svm %>%
  set_engine("kernlab") %>%
  fit_xy(
    x = x_bin_train,
    y = factor(x_bin_train_y)
  )

# Evaluate the SVM model on the test data
test_results <- 
  x_bin_test_y %>%
  bind_cols(
    predict(svm_fit, new_data = x_bin_test)) %>%
  setNames(c('True class', 'Predicted class'))
  

# Compute the confusion matrix and evaluate the SVM performance
caret::confusionMatrix(factor(test_results[['Predicted class']]), factor(test_results[['True class']]))

# Generate PCA plot with svm misclassification information
pca <- prcomp(x_bin_test, center = TRUE, scale=FALSE)
correctly_classified <- test_results$'Predicted class' == test_results$'True class'

x_bin_test_norm <- bake(SVM_recipe, x_bin_test)

f <- kernelMatrix(rbfkernel, as.matrix(x_bin_test_norm))

pca <- prcomp(f, center = TRUE, scale=TRUE)

plot_pca(pca, correctly_classified)


```

```{R roc}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}

simple_roc(as.numeric(test_results$`True class`), as.numeric(test_results$`Predicted class`))








if (!requireNamespace("pROC", quietly = TRUE))
  install.packages("pROC")

library(pROC)

pROC_obj <- roc(as.numeric(test_results$`True class`), as.numeric(test_results$`Predicted class`),
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)


sens.ci <- ci.se(pROC_obj)

plot(sens.ci, type="bars")
```